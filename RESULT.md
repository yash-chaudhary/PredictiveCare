
## Results from Jupyter Notebooks
Upon the pre-processing of the trainig and testing data, the machine learning algorithms, Naive Bayes, Random Forest and Decision Tree were used to fit to the dataset. The training data was used to train the classification models whilst the testing set was the completely new data used to evaluate the model. The performance of the models were based of metrics including precision, accuracy, recall and an F1 score. The accuracy metric determines the number of correctly predicted data points out of all of the data points. This means that if we have a high accuracy, we have the best model. Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. Recall is the ratio of correctly predicted positive observations to all the observations. In other words it measures how many predictions we labelled correctly. And lastly the F1-score is the weighted average of precision and recall. Although the use of these performance metrics sounds great. All the models tested in jupyter notebooks have an accuracy, recall, precision and F1-score of 100%, suggesting that all algorithms are equally great for this dataset. However, despite the very similar performance, some conclusions can be made. The diagonal for the confusion matrix for all 3 machine learning algorithms were all 1, meaning the true label was predicted all the time as also seen by the performance metrics.

Firstly, despite all models having 100% accuracy, this indicates that the learnt rules are specific for the training datasets and do not generalise for the testing dataset. In other words, due to the large size of the training set and the small size of the testing set we cannot be sure that overfitting has not occured. Overfitting when the training dataset is larger than the testing dataset, this means that the model exactly fits against the training dataset. As a result of this, the label is highly dependent on the features, i.e. very specific features will lead to a better diagnosis, this means it probably won't give an accurate predicted diagnosis if all the symptoms selected are not entirely relevant to the actual diagnosis. The case of overfitting was not addressed in Jupyter notebooks as Automated ML in Azure Machine Learning Studio was used to get a second opinion on the dataset and its ability to fit models to it.

Before we move on to explain the results of Azure's Automated ML, a key lesson was learnt by the analysis done in Jupyter Notebooks. Due to the way the dataset was constructed, it seems that better data would have done the machine learning models some justice. By this I mean that an ideal dataset would be where the features are the symptoms that a patient is feeling and the label is the doctors' final diagnosis in the end. Instead the dataset we are using is based on doctors noting the symptoms they see related to the diagnosis they make. As a result, the current features in the dataset are highly related to specific labels. This means that if a user goes to get a prediction and add a symptoms that originally did not fall under a possible symptom for a certain disease, then the user will instead get an extreme or totally unrelated prediction than what they are intending to get. This is not only solved my reducing the bias in the training dataset but also including more diseaes where one symptom can be realted to a range of other disease.

Therefore, despite the seemingly low credibility of the models tested and analysed in Jupyter Notebooks, if we had to choose the best performing model it would be using the Decision Tree algorithm since it had a 100% performance metric for accuracy, precison, recall and F1-score whilst also having the lowest time complexity.

## Results from Azure Automated ML
Azure Automated ML helped clear some of the confusion that arose due to the analysis in Jupyter notebooks. The performance metrics of the models seemed too good to be true. Using Azure Machine Learning meant that many different pre-processing techniques would be used on the dataset to get it as clean as possible before it was used to train models. The fact that this process was automated is one crucial aspect that added to the scalability of my final solution. After pre-processing, Azure ran a host of differnt classifcation models to fit to the data. The result from this was that Azure picked the best model to be a MinMaxScalar RandomForest. 

The preliminary metrics can be see below, these include the same metrics we were measuring in Jupyter Notebooks as well as some additional ones. As you can see all values are 1 signying 100% or a "perfect model"
[!Best Azure ML model](https://github.com/yash-chaudhary/careSpot./blob/main/Azure%20Automated%20ML/model_runs/run_7/Screen%20Shot%202021-08-03%20at%204.18.07%20pm.png)

Funnily enough, the worst before model (not by a large margin) was the MinMaxScalar Decision Tree.
[!Worst Azure Ml model](https://github.com/yash-chaudhary/careSpot./blob/main/Azure%20Automated%20ML/model_runs/run_24/Screen%20Shot%202021-08-03%20at%204.42.01%20pm.png)




