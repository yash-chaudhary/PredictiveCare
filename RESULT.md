
## Results from Jupyter Notebooks
Upon the pre-processing of the trainig and testing data, the machine learning algorithms, Naive Bayes, Random Forest and Decision Tree were used to fit to the dataset. The training data was used to train the classification models whilst the testing set was the completely new data used to evaluate the model. The performance of the models were based of metrics including precision, accuracy, recall and an F1 score. The accuracy metric determines the number of correctly predicted data points out of all of the data points. This means that if we have a high accuracy, we have the best model. Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. Recall is the ratio of correctly predicted positive observations to all the observations. In other words it measures how many predictions we labelled correctly. And lastly the F1-score is the weighted average of precision and recall. Although the use of these performance metrics sounds great. All the models tested in jupyter notebooks have an accuracy, recall, precision and F1-score of 100%, suggesting that all algorithms are equally great for this dataset. However, despite the very similar performance, some conclusions can be made. The diagonal for the confusion matrix for all 3 machine learning algorithms were all 1, meaning the true label was predicted all the time as also seen by the performance metrics.

Firstly, despite all models having 100% accuracy, this indicates that the learnt rules are specific for the training datasets and do not generalise for the testing dataset. In other words, due to the large size of the training set and the small size of the testing set we cannot be sure that overfitting has not occured. Overfitting when the training dataset is larger than the testing dataset, this means that the model exactly fits against the training dataset. As a result of this, the label is highly dependent on the features, i.e. very specific features will lead to a better diagnosis, this means it probably won't give an accurate predicted diagnosis if all the symptoms selected are not entirely relevant to the actual diagnosis. The case of overfitting was not addressed in Jupyter notebooks as Automated ML in Azure Machine Learning Studio was used to get a second opinion on the dataset and its ability to fit models to it.

Before we move on to explain the results of Azure's Automated ML, a key lesson was learnt by the analysis done in Jupyter Notebooks. Due to the way the dataset was constructed, it seems that better data would have done the machine learning models some justice. By this I mean that an ideal dataset would be where the features are the symptoms that a patient is feeling and the label is the doctors' final diagnosis in the end. Instead the dataset we are using is based on doctors noting the symptoms they see related to the diagnosis they make. As a result, the current features in the dataset are highly related to specific labels. This means that if a user goes to get a prediction and add a symptoms that originally did not fall under a possible symptom for a certain disease, then the user will instead get an extreme or totally unrelated prediction than what they are intending to get. This is not only solved my reducing the bias in the training dataset but also including more diseaes where one symptom can be realted to a range of other disease.

Therefore, despite the seemingly low credibility of the models tested and analysed in Jupyter Notebooks, if we had to choose the best performing model it would be using the Decision Tree algorithm since it had a 100% performance metric for accuracy, precison, recall and F1-score whilst also having the lowest time complexity.

## Results from Azure Automated ML
Azure Automated ML helped clear some of the confusion that arose due to the analysis in Jupyter notebooks. The performance metrics of the models seemed too good to be true. Using Azure Machine Learning meant that many different pre-processing techniques would be used on the dataset to get it as clean as possible before it was used to train models. The fact that this process was automated is one crucial aspect that added to the scalability of my final solution. After pre-processing, Azure ran a host of differnt classifcation models to fit to the data. The result from this was that Azure picked the best model to be a MinMaxScalar RandomForest. 

The preliminary metrics can be see below, these include the same metrics we were measuring in Jupyter Notebooks as well as some additional ones. As you can see all values are 1 signifying 100% or a "perfect model"

![Best Azure ML model](https://github.com/yash-chaudhary/careSpot./blob/main/Azure%20Automated%20ML/model_runs/run_7/Screen%20Shot%202021-08-03%20at%204.27.48%20pm.png)


The confusion matrix of the best performing (Random Forest) is pretty much identical to the confusion matrix seen in Jupyter notebooks

![Best Confusion Matrix](https://github.com/yash-chaudhary/careSpot./blob/main/Azure%20Automated%20ML/model_runs/run_7/Screen%20Shot%202021-08-03%20at%204.28.53%20pm.png)


Funnily enough, the worst before model (not by a large margin) was the MinMaxScalar Decision Tree.

![Worst Azure Ml model](https://github.com/yash-chaudhary/careSpot./blob/main/Azure%20Automated%20ML/model_runs/run_24/Screen%20Shot%202021-08-03%20at%204.42.01%20pm.png)

Here is also the confusion matrix of the Desision Tree algorithm. This matrix although still fairly accuracte as seen my the digonal line, still seems more reflective of what a we expect the best model confusion matrix, i.e. not perfectly diagonal but still very accurate. However the fact that this model wasn't able to achive 100% accuracy given the dataset, it must be deemed the worst out of the test.


![Worst Confusion Matrix](https://github.com/yash-chaudhary/careSpot./blob/main/Azure%20Automated%20ML/model_runs/run_24/Screen%20Shot%202021-08-03%20at%204.43.00%20pm.png)


A very important aspect of machine learning that aims to be considered in the future is feature engineering. Feature engineering involves the selection of features based on their importance in the dataset. Azure has graphed for us the feature importance in the dataset given the Random Forest (best algorithm).

![Fearure importance graph](https://github.com/yash-chaudhary/careSpot./blob/main/Azure%20Automated%20ML/model_runs/run_7/Screen%20Shot%202021-08-03%20at%204.37.30%20pm.png)


As you can see above, there are several features that bear no influence on the data. Hence to improve this solution, feature engineering should be done in the future where we select a subset of the most important features to better represent the data, hence making solution more understandable and easier to use and may even lead to better, more accurate predictions with the side benefit of smaller dataset which can be trained faster. In the end the model that was selected was the MinMaxScalar Random Forest that Automated ML picked. The obvious reasons for this are that it was the most accurate model out of the three that were analysed and moreover in the pre-run configurations, I had asked Azure to benchmark all classification models against each other. Hence the many pre-processing that Azure did coupled with the comparison of a vast number of ML models, roughly 43 runs, deduced that the MinMaxScalar Random Forest was the best model. This conclusion couldn't be contested since in Jupyter Notebooks, only one version of each type of classifcation model was tested and yielded information that wasn't super helpful to determine a clear best performing model.

## Reflection
The machine learning lifecyles and machine learning in general is something very new to me and I'm glad that I have been able to learn more about this intersting field of study. I plan to explore ML and AI more from a maths and stats standpoint before implementing them into my future projects. I've been able to see the vast benefits of Azure Machine Learning Studio in terms of how easy it is to trian a model giving you have a dataset. I am extremely happy with my solution. The design came out exactly how I envisioned as I spent many hours with custom styles trying to make the solution live up to my standards. Not only have I learnt how to make a flask web app but I feel I've accomplished a true end-to-end solution using technology such as the cloud, fullstack web development and machine learning and AI. One of my key take-aways is the importance of a really good dataset. Although the predictions work there is plenty of room for improvement in the ML/AI arena to make a model that will likely more suitable for users as well as more accurate.

